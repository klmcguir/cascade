{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot reconstructed data heatmap next to actual data heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import tensortools as tt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from cascade import df\n",
    "from cascade import tca\n",
    "from cascade import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlot reconstruction error as variance explained across all days for\\nTCA decomposition ensembles.\\n\\nParameters:\\n-----------\\nmouse : str; mouse name\\ntrace_type : str; dff, zscore, deconvolved\\nmethod : str; TCA fit method from tensortools\\n\\nReturns:\\n--------\\nSaves figures to .../analysis folder  .../qc\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def singleday_varex_summary(\n",
    "mouse = 'OA27'\n",
    "trace_type='zscore_day'\n",
    "method='ncp_bcd'\n",
    "cs=''\n",
    "warp=False\n",
    "word='ratios'\n",
    "verbose=False\n",
    "r = 10\n",
    "\"\"\"\n",
    "Plot reconstruction error as variance explained across all days for\n",
    "TCA decomposition ensembles.\n",
    "\n",
    "Parameters:\n",
    "-----------\n",
    "mouse : str; mouse name\n",
    "trace_type : str; dff, zscore, deconvolved\n",
    "method : str; TCA fit method from tensortools\n",
    "\n",
    "Returns:\n",
    "--------\n",
    "Saves figures to .../analysis folder  .../qc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = {'trace_type': trace_type, 'cs': cs, 'warp': warp}\n",
    "\n",
    "days = flow.metadata.DateSorter.frommeta(mice=[mouse], tags=None)\n",
    "\n",
    "cmap = sns.color_palette(sns.cubehelix_palette(len(days)))\n",
    "\n",
    "for c, day1 in enumerate(days, 0):\n",
    "    \n",
    "    # create figure and axes\n",
    "    buffer = 5\n",
    "    right_pad = 5\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    gs = GridSpec(100, 100, figure=fig, left=0.05, right=.95, top=.95, bottom=0.05)\n",
    "    ax0 = fig.add_subplot(gs[10:90-buffer, 5:45-right_pad])\n",
    "    ax1 = fig.add_subplot(gs[10:90-buffer, 50:90-right_pad])\n",
    "#     axC = fig.add_subplot(gs[20:80-buffer, 97:98])\n",
    "\n",
    "#     day1 = day[0]\n",
    "    # load dirs\n",
    "    load_dir = paths.tca_path(mouse, 'single', pars=pars, word=word)\n",
    "    tensor_path = os.path.join(load_dir, str(day1.mouse) + '_'\n",
    "                               + str(day1.date) + '_single_decomp_'\n",
    "                               + str(trace_type) + '.npy')\n",
    "    input_tensor_path = os.path.join(load_dir, str(day1.mouse) + '_'\n",
    "                                     + str(day1.date) + '_single_tensor_'\n",
    "                                     + str(trace_type) + '.npy')\n",
    "    if not os.path.isfile(tensor_path): continue\n",
    "    if not os.path.isfile(input_tensor_path): continue\n",
    "\n",
    "    # save dirs\n",
    "    save_dir = paths.tca_plots(mouse, 'single', pars=pars, word=word)\n",
    "    save_dir = os.path.join(save_dir, 'reconstructions')\n",
    "    if not os.path.isdir(save_dir): os.mkdir(save_dir)\n",
    "    var_path = os.path.join(save_dir, str(day1.mouse) + '_'\n",
    "                               + str(day1.date) + '_recon_heatmap.pdf')\n",
    "\n",
    "    # load your data\n",
    "    ensemble = np.load(tensor_path)\n",
    "    ensemble = ensemble.item()\n",
    "    V = ensemble[method]\n",
    "    X = np.load(input_tensor_path)\n",
    "    bU = V.results[r][0].factors.full()\n",
    "    \n",
    "#     plt.figure()\n",
    "    # red=high, white=middle, blue=low colormap\n",
    "    vmax = np.nanmax(np.concatenate((np.nanmean(bU[:,:,:],2).flatten(), np.nanmean(X[:,:,:],2).flatten())))\n",
    "#     cmap = sns.light_palette((10, 80, 50), input=\"husl\", as_cmap=True)\n",
    "#     cmap\n",
    "    im0 = ax0.imshow(np.nanmean(X[:,:,:],2), vmin=0, vmax=vmax)\n",
    "    fig.colorbar(im0, ax=ax0)\n",
    "#     plt.figure()\n",
    "    im1 = ax1.imshow(np.nanmean(bU[:,:,:],2), vmin=0, vmax=vmax)\n",
    "    fig.colorbar(im1, ax=ax1)\n",
    "\n",
    "    ax0.set_title('Data')\n",
    "    ax0.set_ylabel('Cell Number')\n",
    "    ax0.set_xlabel('Time (sec)')\n",
    "    ax1.set_title('TCA Model')\n",
    "    ax1.set_ylabel('Cell Number')\n",
    "    ax1.set_xlabel('Time (sec)')\n",
    "    \n",
    "    # reset xticklabels\n",
    "    xticklabels = np.array([0, 1, 2, 3, 4, 5])\n",
    "#     xticklabels = xticklabels[(xticklabels > times[0]) & (xticklabels < times[-1])]\n",
    "    xticks = np.array([15, 30, 45, 60, 75, 90])\n",
    "    ax1.set_xticks(xticks)\n",
    "    ax1.set_xticklabels(xticklabels, rotation='horizontal')\n",
    "    ax0.set_xticks(xticks)\n",
    "    ax0.set_xticklabels(xticklabels, rotation='horizontal')\n",
    "    fig.savefig(var_path, bbox_inches='tight')\n",
    "#     vmin, vmax = ax1.clim()\n",
    "#     ax1.set_clim(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    \n",
    "    # get reconstruction error as variance explained\n",
    "#     var, var_s, x, x_s = [], [], [], []\n",
    "# #     for r in V.results\n",
    "#     bU = V.results[r][0].factors.full()\n",
    "#     var.append((np.var(X) - np.var(X - bU)) / np.var(X))\n",
    "#     x.append(r)\n",
    "#     for it in range(0, len(V.results[r])):\n",
    "#         U = V.results[r][it].factors.full()\n",
    "#         var_s.extend([(np.var(X) - np.var(X - U)) / np.var(X)])\n",
    "#         x_s.extend([r])\n",
    "#     break\n",
    "\n",
    "#     # mean response of neuron across trials\n",
    "#     mU = np.mean(X, axis=2, keepdims=True) * np.ones((1, 1, np.shape(X)[2]))\n",
    "#     var_mean = (np.var(X) - np.var(X - mU)) / np.var(X)\n",
    "\n",
    "#     # smoothed response of neuron across time\n",
    "#     smU = np.convolve(X.reshape((X.size)), np.ones(5, dtype=np.float64)/5, 'same').reshape(np.shape(X))\n",
    "#     var_smooth = (np.var(X) - np.var(X - smU)) / np.var(X)\n",
    "\n",
    "    # plot\n",
    "#     R = np.max([r for r in V.results.keys()])\n",
    "#     ax.scatter(x_s, var_s, color=cmap[c], alpha=0.5)\n",
    "#     ax.scatter([R+2], var_mean, color=cmap[c], alpha=0.5)\n",
    "#     ax.scatter([R+4], var_smooth, color=cmap[c], alpha=0.5)\n",
    "#     ax.plot(x, var, label=('single ' + str(c)), color=cmap[c])\n",
    "#     ax.plot([R+1.5, R+2.5], [var_mean, var_mean], color=cmap[c])\n",
    "#     ax.plot([R+3.5, R+4.5], [var_smooth, var_smooth], color=cmap[c])\n",
    "\n",
    "# # add labels/titles\n",
    "# x_labels = [str(R) for R in V.results]\n",
    "# x_labels.extend(['', 'mean\\n cell\\n response', '', 'smooth\\n response\\n (0.3s)'])\n",
    "# ax.set_xticks(range(1, len(V.results) + 5))\n",
    "# ax.set_xticklabels(x_labels)\n",
    "# ax.set_xlabel('model rank')\n",
    "# ax.set_ylabel('fractional variance explained')\n",
    "# ax.set_title('Variance Explained: ' + str(method) + ', ' + mouse)\n",
    "# ax.legend(bbox_to_anchor=(1.03, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "# fig.savefig(var_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABLCAYAAABHlv13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAAgdJREFUeJzt27FqFFEUx+FzJTaxNVoKQrANJM+0D+WjaGEtmC6dQV8gnSDZFXIstN9dZZj/Xr6vG7gs5zDsr7gwo7sLgPU9W3sAAP4QZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhDjbd2CMsamqTVXVi/Pz63eXl4sPtZZ+3K49wqJ6O+9+M+9WVdWPu7VHWNTT5O/v7uePh+6+2HduHPPp9PXVVX/+8PG/Bku2+/pt7REWtbv/vvYIi9ndz/3ufk2+3+z/vbe3n267+2bfOVcWACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEGf7DowxNlW1+fu4ff761d2yI63qZVU9rD3EQmbercp+p272/d4ccmh098G/OMb40t03/zxSuJn3m3m3Kvudutn3O5QrC4AQggwQ4tggv19kihwz7zfzblX2O3Wz73eQo+6QAViOKwuAEIIMEEKQAUIIMkAIQQYI8RsTcV4Ug9fA5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.palplot(sns.light_palette((10, 80, 50), input=\"husl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
