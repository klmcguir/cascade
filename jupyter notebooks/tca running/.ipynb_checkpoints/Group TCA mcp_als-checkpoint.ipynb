{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing mcp_als on larger decompositions for groups of days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensortools as tt\n",
    "import numpy as np\n",
    "import flow\n",
    "from flow.misc import wordhash\n",
    "import pool\n",
    "import pandas as pd\n",
    "import os\n",
    "from cascade import utils\n",
    "from cascade import paths\n",
    "from cascade import tca\n",
    "from copy import deepcopy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse='OA27'\n",
    "tags=None\n",
    "\n",
    "# TCA params\n",
    "rank=20\n",
    "method=('mcp_als',)\n",
    "replicates=3\n",
    "fit_options=None\n",
    "\n",
    "# grouping params\n",
    "group_by='l_vs_r1'\n",
    "up_or_down='up'\n",
    "use_dprime=False\n",
    "dprime_threshold=2\n",
    "\n",
    "# tensor params\n",
    "trace_type='zscore_day'\n",
    "cs=''\n",
    "downsample=True\n",
    "start_time=-1\n",
    "end_time=6\n",
    "clean_artifacts=None\n",
    "thresh=20\n",
    "warp=False\n",
    "smooth=True\n",
    "smooth_win=5\n",
    "verbose=True\n",
    "\n",
    "# filtering params\n",
    "exclude_tags=('disengaged', 'orientation_mapping', 'contrast', 'retinotopy', 'sated')\n",
    "exclude_conds=('blank', 'blank_reward', 'pavlovian')\n",
    "driven=True\n",
    "drive_css=('0', '135', '270')\n",
    "drive_threshold=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCA parameters hashed: divorce\n",
      "Tensor decomp about to begin: tensor shape = (728, 108, 5103)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform tensor component analysis (TCA) on data aligned\n",
    "across a group of days. Builds one large tensor.\n",
    "\n",
    "Algortitms from https://github.com/ahwillia/tensortools.\n",
    "\n",
    "Parameters\n",
    "-------\n",
    "methods, tuple of str\n",
    "'cp_als', fits CP Decomposition using Alternating\n",
    "    Least Squares (ALS).\n",
    "'ncp_bcd', fits nonnegative CP Decomposition using\n",
    "    the Block Coordinate Descent (BCD) Method.\n",
    "'ncp_hals', fits nonnegtaive CP Decomposition using\n",
    "    the Hierarcial Alternating Least Squares\n",
    "    (HALS) Method.\n",
    "'mcp_als', fits CP Decomposition with missing data using\n",
    "    Alternating Least Squares (ALS).\n",
    "\n",
    "rank, int\n",
    "number of components you wish to fit\n",
    "\n",
    "replicates, int\n",
    "number of initializations/iterations fitting for each rank\n",
    "\n",
    "Returns\n",
    "-------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# set grouping parameters\n",
    "if group_by.lower() == 'naive':\n",
    "    tags = 'naive'\n",
    "    use_dprime = False\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated', 'learning_start')\n",
    "\n",
    "elif group_by.lower() == 'high_dprime_learning':\n",
    "    use_dprime = True\n",
    "    up_or_down = 'up'\n",
    "    tags = 'learning'\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated', 'reversal1_start')\n",
    "\n",
    "elif group_by.lower() == 'low_dprime_leanring':\n",
    "    use_dprime = True\n",
    "    up_or_down = 'down'\n",
    "    tags = 'learning'\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated')\n",
    "\n",
    "elif group_by.lower() == 'high_dprime_reversal1':\n",
    "    use_dprime = True\n",
    "    up_or_down = 'up'\n",
    "    tags = 'reversal1'\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated', 'reversal2_start')\n",
    "\n",
    "elif group_by.lower() == 'low_dprime_reversal1':\n",
    "    use_dprime = True\n",
    "    up_or_down = 'down'\n",
    "    tags = 'reversal1'\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated')\n",
    "\n",
    "elif group_by.lower() == 'high_dprime_reversal2':\n",
    "    use_dprime = True\n",
    "    up_or_down = 'up'\n",
    "    tags = 'reversal2'\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated')\n",
    "\n",
    "elif group_by.lower() == 'low_dprime_reversal2':\n",
    "    use_dprime = True\n",
    "    up_or_down = 'down'\n",
    "    tags = 'reversal2'\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated')\n",
    "\n",
    "elif group_by.lower() == 'naive_vs_high_dprime':\n",
    "    use_dprime = True\n",
    "    up_or_down = 'up'\n",
    "    tags = None\n",
    "    days = flow.DateSorter.frommeta(mice=[mouse], tags='naive')\n",
    "    days.extend(flow.DateSorter.frommeta(mice=[mouse], tags='learning'))\n",
    "    dates = set(days)\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated', 'learning_start',\n",
    "                    'reversal1_start')\n",
    "elif group_by.lower() == 'l_vs_r1':  # high dprime\n",
    "    use_dprime = True\n",
    "    up_or_down = 'up'\n",
    "    tags = None\n",
    "    days = flow.DateSorter.frommeta(mice=[mouse], tags='learning')\n",
    "    days.extend(flow.DateSorter.frommeta(mice=[mouse], tags='reversal1'))\n",
    "    dates = set(days)\n",
    "    exclude_tags = ('disengaged', 'orientation_mapping', 'contrast',\n",
    "                    'retinotopy', 'sated', 'learning_start',\n",
    "                    'reversal1_start')\n",
    "else:\n",
    "    print('Using input parameters without modification by group_by=...')\n",
    "\n",
    "# create folder structure and save dir\n",
    "if fit_options is None:\n",
    "    fit_options = {'tol': 0.0001, 'max_iter': 500, 'verbose': False}\n",
    "pars = {'tags': tags, 'rank': rank, 'method': method,\n",
    "        'replicates': replicates, 'fit_options': fit_options,\n",
    "        'trace_type': trace_type, 'cs': cs, 'downsample': downsample,\n",
    "        'start_time': start_time, 'end_time': end_time,\n",
    "        'clean_artifacts': clean_artifacts, 'thresh': thresh,\n",
    "        'warp': warp, 'smooth': smooth, 'smooth_win': smooth_win,\n",
    "        'exclude_tags': exclude_tags, 'exclude_conds': exclude_conds,\n",
    "        'driven': driven, 'drive_css': drive_css,\n",
    "        'drive_threshold': drive_threshold}\n",
    "group_pars = {'group_by': group_by, 'up_or_down': up_or_down,\n",
    "              'use_dprime': use_dprime, 'dprime_threshold': dprime_threshold}\n",
    "save_dir = paths.tca_path(mouse, 'group', pars=pars, group_pars=group_pars)\n",
    "\n",
    "# get DateSorter object\n",
    "if np.isin(group_by.lower(), ['naive_vs_high_dprime', 'l_vs_r1']):\n",
    "    days = flow.DateSorter(dates=dates)\n",
    "else:\n",
    "    days = flow.DateSorter.frommeta(mice=[mouse], tags=tags)\n",
    "\n",
    "# filter DateSorter object if you are filtering on dprime\n",
    "if use_dprime:\n",
    "    dprime = []\n",
    "    for day1 in days:\n",
    "        # for comparison with naive make sure dprime keeps naive days\n",
    "        if np.isin('naive', day1.tags):\n",
    "            if up_or_down.lower() == 'up':\n",
    "                dprime.append(np.inf)\n",
    "            else:\n",
    "                dprime.append(-np.inf)\n",
    "        else:\n",
    "            dprime.append(pool.calc.performance.dprime(day1))\n",
    "    if up_or_down.lower() == 'up':\n",
    "        days = [d for c, d in enumerate(days) if dprime[c] > dprime_threshold]\n",
    "    elif up_or_down.lower() == 'down':\n",
    "        days = [d for c, d in enumerate(days) if dprime[c] <= dprime_threshold]\n",
    "\n",
    "# preallocate for looping over a group of days/runs\n",
    "meta_list = []\n",
    "tensor_list = []\n",
    "id_list = []\n",
    "for c, day1 in enumerate(days, 0):\n",
    "\n",
    "    # get cell_ids\n",
    "    d1_ids = flow.xday._read_crossday_ids(day1.mouse, day1.date)\n",
    "    d1_ids = np.array([int(s) for s in d1_ids])\n",
    "\n",
    "    # filter cells based on visual/trial drive across all cs, prevent\n",
    "    # breaking when only pavs are shown\n",
    "    if driven:\n",
    "        good_ids = tca._group_drive_ids(days, drive_css, drive_threshold)\n",
    "        d1_ids_bool = np.isin(d1_ids, good_ids)\n",
    "        d1_sorter = np.argsort(d1_ids[d1_ids_bool])\n",
    "    else:\n",
    "        d1_ids_bool = np.ones(np.shape(d1_ids)) > 0\n",
    "        d1_sorter = np.argsort(d1_ids[d1_ids_bool])\n",
    "    ids = d1_ids[d1_ids_bool][d1_sorter]\n",
    "\n",
    "    # TODO add in additional filter for being able to check for quality of xday alignment\n",
    "\n",
    "    # get all runs for both days\n",
    "    d1_runs = day1.runs()\n",
    "\n",
    "    # filter for only runs without certain tags\n",
    "    d1_runs = [run for run in d1_runs if not any(np.isin(run.tags, exclude_tags))]\n",
    "\n",
    "    # build tensors for all correct runs and trials after filtering\n",
    "    if d1_runs:\n",
    "        d1_tensor_list = []\n",
    "        d1_meta = []\n",
    "        for run in d1_runs:\n",
    "            t2p = run.trace2p()\n",
    "            # trigger all trials around stimulus onsets\n",
    "            run_traces = utils.getcstraces(run, cs=cs, trace_type=trace_type,\n",
    "                                     start_time=start_time, end_time=end_time,\n",
    "                                     downsample=True, clean_artifacts=clean_artifacts,\n",
    "                                     thresh=thresh, warp=warp, smooth=smooth,\n",
    "                                     smooth_win=smooth_win)\n",
    "            # filter and sort\n",
    "            run_traces = run_traces[d1_ids_bool, :, :][d1_sorter, :, :]\n",
    "            # get matched trial metadata/variables\n",
    "            dfr = tca._trialmetafromrun(run)\n",
    "            # subselect metadata if you are only running certain cs\n",
    "            if cs != '':\n",
    "                if cs == 'plus' or cs == 'minus' or cs == 'neutral':\n",
    "                    dfr = dfr.loc[(dfr['condition'].isin([cs])), :]\n",
    "                elif cs == '0' or cs == '135' or cs == '270':\n",
    "                    dfr = dfr.loc[(dfr['orientation'].isin([cs])), :]\n",
    "                else:\n",
    "                    print('ERROR: cs called - \"' + cs + '\" - is not\\\n",
    "                          a valid option.')\n",
    "\n",
    "            # subselect metadata to remove certain condtions\n",
    "            if len(exclude_conds) > 0:\n",
    "                dfr = dfr.loc[(~dfr['condition'].isin([exclude_conds])), :]\n",
    "\n",
    "            # drop trials with nans and add to lists\n",
    "            keep = np.sum(np.sum(np.isnan(run_traces), axis=0, keepdims=True),\n",
    "                          axis=1, keepdims=True).flatten() == 0\n",
    "            dfr = dfr.iloc[keep, :]\n",
    "            d1_tensor_list.append(run_traces[:, :, keep])\n",
    "            d1_meta.append(dfr)\n",
    "\n",
    "        # concatenate matched cells across trials 3rd dim (aka, 2)\n",
    "        tensor = np.concatenate(d1_tensor_list, axis=2)\n",
    "\n",
    "        # concatenate all trial metadata in pd dataframe\n",
    "        meta = pd.concat(d1_meta, axis=0)\n",
    "\n",
    "        meta_list.append(meta)\n",
    "        tensor_list.append(tensor)\n",
    "        id_list.append(ids)\n",
    "\n",
    "# get total trial number across all days/runs\n",
    "meta = pd.concat(meta_list, axis=0)\n",
    "trial_num = len(meta.reset_index()['trial_idx'])\n",
    "\n",
    "# get union of ids. Use these for indexing and splicing tensors together\n",
    "id_union = np.unique(np.concatenate(id_list, axis=0))\n",
    "cell_num = len(id_union)\n",
    "\n",
    "# build a single large tensor leaving zeros where cell is not found\n",
    "trial_start = 0\n",
    "trial_end = 0\n",
    "group_tensor = np.zeros((cell_num, np.shape(tensor_list[0])[1], trial_num))\n",
    "group_tensor[:] = np.nan\n",
    "for i in range(len(tensor_list)):\n",
    "    trial_end += np.shape(tensor_list[i])[2]\n",
    "    for c, k in enumerate(id_list[i]):\n",
    "        celln_all_trials = tensor_list[i][c, :, :]\n",
    "        group_tensor[(id_union == k), :, trial_start:trial_end] = celln_all_trials\n",
    "    trial_start += np.shape(tensor_list[i])[2]\n",
    "\n",
    "# just so you have a clue how big the tensor is\n",
    "print('Tensor decomp about to begin: tensor shape = '\n",
    "      + str(np.shape(group_tensor)))\n",
    "\n",
    "# # concatenate and save df for the day\n",
    "# meta_path = os.path.join(save_dir, str(day1.mouse) + '_'\n",
    "#                          + str(group_by) + '_df_group_meta.pkl')\n",
    "# input_tensor_path = os.path.join(save_dir, str(day1.mouse) + '_'\n",
    "#                                  + str(group_by) + '_group_tensor_'\n",
    "#                                  + str(trace_type) + '.npy')\n",
    "# input_ids_path = os.path.join(save_dir, str(day1.mouse) + '_'\n",
    "#                               + str(group_by) + '_group_ids_'\n",
    "#                               + str(trace_type) + '.npy')\n",
    "# output_tensor_path = os.path.join(save_dir, str(day1.mouse) + '_'\n",
    "#                                   + str(group_by) + '_group_decomp_'\n",
    "#                                   + str(trace_type) + '.npy')\n",
    "# meta.to_pickle(meta_path)\n",
    "# np.save(input_tensor_path, group_tensor)\n",
    "# np.save(input_ids_path, id_union)\n",
    "\n",
    "# # run TCA - iterate over different fitting methods\n",
    "# if np.isin('mcp_als', fit_method):\n",
    "#     mask = np.ones((cell_num, np.shape(tensor_list[0])[1], trial_num))\n",
    "#     mask[np.isnan(group_tensor)] = 0\n",
    "#     group_tensor[np.isnan(group_tensor)] = 0\n",
    "#     mcp_als(group_tensor, rank, mask, **fit_options)\n",
    "# else:\n",
    "#     ensemble = {}\n",
    "#     for m in method:\n",
    "#         ensemble[m] = tt.Ensemble(\n",
    "#             fit_method=m, fit_options=deepcopy(fit_options))\n",
    "#         ensemble[m].fit(group_tensor, ranks=range(1, rank+1), replicates=replicates, verbose=False)\n",
    "#     np.save(output_tensor_path, ensemble)\n",
    "\n",
    "# # print output so you don't go crazy waiting\n",
    "# if verbose:\n",
    "#     print(str(day1.mouse) + ': group_by=' + str(group_by) + ': done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yep\n"
     ]
    }
   ],
   "source": [
    "# run TCA - iterate over different fitting methods\n",
    "if np.isin('mcp_als', method):\n",
    "    print('yep')\n",
    "    mask = np.ones((cell_num, np.shape(tensor_list[0])[1], trial_num))\n",
    "    mask[np.isnan(group_tensor)] = 0\n",
    "    group_tensor[np.isnan(group_tensor)] = 0\n",
    "    test = tt.mcp_als(group_tensor, 10, mask, **fit_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
